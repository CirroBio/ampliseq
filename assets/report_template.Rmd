---
title: Summary of nf-core/ampliseq results
output:
    html_document:
        toc: true # table of contents
        toc_float: true # float the table of contents to the left of the main document content
        toc_depth: 3 # header levels 1,2,3
        theme: default
        number_sections: true # add section numbering to headers
        df_print: paged # tables are printed as an html table with support for pagination over rows and columns
        css: ./report_styles.css
        highlight: pygments
    pdf_document: true
#bibliography: ./references.bibtex
params:
    # flags and arguments
    flag_skip_fastqc: FALSE
    flag_skip_cutadapt: FALSE
    flag_skip_dada_quality: FALSE
    flag_skip_barrnap: FALSE
    #flag_skip_taxonomy: FALSE
    flag_retain_untrimmed: TRUE
    flag_ref_tax_user: FALSE
    flag_single_end: FALSE
    flag_dada2_taxonomy: FALSE
    flag_qiime2_taxonomy: FALSE
    flag_sintax_taxonomy: FALSE
    flag_pplace_taxonomy: FALSE
    trunclenf: ""
    trunclenr: ""
    max_ee: ""
    trunc_qmin: ""
    trunc_rmin: ""

    # file paths
    mqc_plot: ""
    ca_sum_path: ""
    dada_filtntrim_args: ""
    dada_qc_f_path: ""
    dada_qc_r_path: ""
    dada_pp_qc_f_path: ""
    dada_pp_qc_r_path: ""
    dada_1_err_path: ""
    dada_2_err_path: ""
    asv_table_path: ""
    path_asv_fa: ""
    path_dada2_tab: ""
    dada_stats_path: ""
    path_rrna_arc: ""
    path_rrna_bac: ""
    path_rrna_euk: ""
    path_rrna_mito: ""
    path_barrnap_sum: ""
    ref_tax_path: ""
    dada2_taxonomy: "empty"
    sintax_taxonomy: ""
    pplace_taxonomy: ""
    qiime2_taxonomy: ""

---

```{r setup, include=FALSE}
library("dplyr")
library("ggplot2")
library("knitr")
library("DT")
library("formattable")
library("purrr")
knitr::opts_chunk$set(echo = FALSE)
```

# Preprocessing

```{r, eval = !params$flag_skip_fastqc, results='asis'}
mqc_rep_path <- paste0("../multiqc/multiqc_report.html")

cat("## FastQC\n")
cat("The sequence quality was checked using FastQC and resulting data was ",
        "aggregated using the FastQC module of MultiQC. For more quality ",
        "controls and per sample quality checks you can check the full ",
        "MultiQC report, which is found [here](", mqc_rep_path, ").", sep = "")
```

```{r, eval = !params$flag_skip_fastqc, out.width='100%', dpi=1200, fig.align='center'}
knitr::include_graphics(params$mqc_plot)
```

```{r, eval = !params$flag_skip_cutadapt, results='asis'}
# import tsv
cutadapt_summary <- read.table(file = params$ca_sum_path, header = TRUE, sep = "\t")

passed_col <- as.numeric(substr(
        cutadapt_summary$cutadapt_passing_filters_percent, 1, 4))

max_disc <- 100 - min(passed_col)
avg_passed <- mean(passed_col)

cutadapt_text_unch <- paste0("## Cutadapt\n",
        "Remaining primers were trimmed using cutadapt")
cutadapt_text_ch <- paste0("and all untrimmed sequences were discarded. <",
        max_disc, "% of the sequences were discarded per sample and a mean of ",
        avg_passed, "% of the sequences per sample passed the filtering.")

if (!params$flag_retain_untrimmed) cutadapt_text <- paste0(
    cutadapt_text_unch, cutadapt_text_ch
    ) else cutadapt_text <- paste0(cutadapt_text_unch, ".")

cat(cutadapt_text)

datatable(cutadapt_summary, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))

cutadapt_summary$passed_num <- passed_col

ggplot(cutadapt_summary,
        aes(x = sample, y = passed_col)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% passing filters of cutadapt") +
        xlab("Samples") +
        coord_flip() +
        theme_bw()
```

```{r, results='asis'}
cat("## QC using DADA2\n")
if (params$trunc_qmin != -1) {
    f_and_tr_args <- readLines(params$dada_filtntrim_args)
    trunc_len <- strsplit(gsub(".*truncLen = c\\((.+)\\),maxN.*", "\\1",
                            f_and_tr_args), ", ")
    tr_len_f <- trunc_len[[1]][1]
    tr_len_r <- trunc_len[[1]][2]
    cat("Reads were trimmed before median quality drops ",
        "below ", params$trunc_qmin, " and at least ",params$trunc_rmin*100,
        "% of reads are retained, resulting in a trim of ",
        "forward reads at ", tr_len_f, " bp and reverse ",
        "reads at ", tr_len_r, " bp. ", sep = "")
} else if (params$trunclenf == "null" && params$trunclenr == "null") {
    cat("Reads were not trimmed.")
} else if (params$trunclenf != 0 && params$trunclenr != 0) {
    cat("Forward reads were trimmed at ", params$trunclenf,
            " bp and reverse reads were trimmed at ", params$trunclenr,
            " bp. ", sep = "")
} else if (params$trunclenf != 0) {
    cat("Forward reads were trimmed at ", params$trunclenf," bp. ", sep = "")
} else if (params$trunclenr != 0) {
    cat("Reverse reads were trimmed at ", params$trunclenr," bp. ", sep = "")
}
cat("Reads with more than", params$max_ee,"expected errors were discarded.", sep = " ")
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
if (params$flag_single_end) {
    cat("Read quality stats for incoming data:")
} else {
    cat("Forward (left) and reverse (right) read quality stats for incoming data:")
}
```

```{r, eval = !params$flag_skip_dada_quality, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_qc_f_path, params$dada_qc_r_path))
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
if (params$flag_single_end) {
    cat("Read quality stats for preprocessed data:")
} else {
    cat("Forward (left) and reverse (right) read quality stats for preprocessed data:")
}
```

```{r, eval = !params$flag_skip_dada_quality, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_pp_qc_f_path, params$dada_pp_qc_r_path))
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
cat("Original plots can be found [here](../dada2/QC/).")
```

## Error correction using DADA2

Error correction was performed using DADA2 as well and the originalplots can be found [here](../dada2/QC/).

```{r, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_1_err_path, params$dada_2_err_path))
```

```{r, results='asis'}
# Header
cat("## ASV inference using DADA2\n")

#import asv table
asv_table <- read.table(file = params$asv_table_path, header = TRUE, sep = "\t")
n_asv <- length(asv_table$ASV_ID)

# Output text
cat(n_asv,
        "amplicon sequence variants (ASVs) were obtained across all samples. ")
cat("The ASVs can be found in ['ASV_seqs.fasta'](../dada2/). And the corresponding",
        " quantification of the ASVs across samples can be found in",
        "['ASV_table.tsv'](../dada2/). An extensive table containing both can ",
        "be found ['DADA2_table.tsv'](../dada2/)")

# import stats tsv
dada_stats <- read.table(file = params$dada_stats_path, header = TRUE, sep = "\t")

# Display table
datatable(dada_stats, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))

# Stacked barchart to num of reads

# Calc exluded asvs and transform all cols to percent

if ( params$flag_single_end ) {
    # single end
    dada_stats_ex <- data.frame(sample = dada_stats$sample,
                            input = dada_stats$DADA2_input,
                            filtered = dada_stats$DADA2_input-dada_stats$filtered,
                            denoised = dada_stats$filtered-dada_stats$denoised,
                            nonchim = dada_stats$denoised-dada_stats$nonchim,
                            analysis = dada_stats$nonchim)
    dada_stats_p <- data.frame(sample = dada_stats_ex$sample, round(dada_stats_ex[2:6]/dada_stats_ex$input, 2))
    n_samples <- length(dada_stats_p$sample)
    # Stack columns for both stacked barcharts
    samples_t <- c(rep(dada_stats_p$sample, 4))
    steps_t <- c(rep("excluded by filtering", n_samples), rep("excluded by denoised", n_samples),
                rep("excluded by nonchim", n_samples), rep("ready for analysis", n_samples))
    # stack the column for absolute number of asvs
    asvs_abs_t <- as.array(flatten_dbl(dada_stats_ex[3:6]))
    dada_stats_ex_t <- data.frame(samples_t, steps_t, asvs_abs_t)
    # stack the column for percentage of asvs
    asvs_p_t <- as.array(flatten_dbl(dada_stats_p[3:6]))
    dada_stats_p_t <- data.frame(samples_t, steps_t, asvs_p_t)
} else {
    # paired end
    dada_stats_ex <- data.frame(sample = dada_stats$sample,
                            DADA2_input = dada_stats$DADA2_input,
                            filtered = dada_stats$DADA2_input-dada_stats$filtered,
                            denoisedF = dada_stats$filtered-dada_stats$denoisedF,
                            denoisedR = dada_stats$denoisedF-dada_stats$denoisedR,
                            merged = dada_stats$denoisedR-dada_stats$merged,
                            nonchim = dada_stats$merged-dada_stats$nonchim,
                            analysis = dada_stats$nonchim)
    dada_stats_p <- data.frame(sample = dada_stats_ex$sample, round(dada_stats_ex[2:8]/dada_stats_ex$DADA2_input, 2))
    # Stack columns for both stacked barcharts
    n_samples <- length(dada_stats_p$sample)
    samples_t <- c(rep(dada_stats_p$sample, 6))
    steps_t <- c(rep("excluded by filtering", n_samples), rep("excluded by denoisedF", n_samples),
                rep("excluded by denoisedR", n_samples), rep("excluded by merged", n_samples),
                rep("excluded by nonchim", n_samples), rep("ready for analysis", n_samples))
    # stack the column for absolute number of asvs
    asvs_abs_t <- as.array(flatten_dbl(dada_stats_ex[3:8]))
    dada_stats_ex_t <- data.frame(samples_t, steps_t, asvs_abs_t)
    # stack the column for percentage of asvs
    asvs_p_t <- as.array(flatten_dbl(dada_stats_p[3:8]))
    dada_stats_p_t <- data.frame(samples_t, steps_t, asvs_p_t)
}

# Plot
dada_stats_ex_t$steps_t <- factor(dada_stats_ex_t$steps_t, levels=unique(dada_stats_ex_t$steps_t))
ggplot(dada_stats_ex_t, aes(fill = steps_t, y = asvs_abs_t, x = samples_t)) +
        geom_bar(position = "stack", stat = "identity") +
        xlab("Samples") +
        ylab("Absolute number ASVs") +
        coord_flip() +
        scale_fill_brewer("Filtering Steps", palette = "Spectral")

# Plot
dada_stats_p_t$steps_t <- factor(dada_stats_p_t$steps_t, levels=unique(dada_stats_p_t$steps_t))
ggplot(dada_stats_p_t, aes(fill = steps_t, y = asvs_p_t, x = samples_t)) +
        geom_bar(position = "fill", stat = "identity") +
        xlab("Samples") +
        ylab("% of total ASVs") +
        coord_flip() +
        scale_fill_brewer("Filtering Steps", palette = "Spectral")
```

```{r, eval = !params$flag_skip_barrnap, results='asis'}
# Header
cat("# Filtering of ASVs\n")
cat("## ASV filtering using Barrnap\n")
cat("Barrnap classifies the ASVs into the origin domain (including ",
        "mitochondiral origin). Using this classification the ASVs can ",
        "be filtered by sample origin.\n\n", sep = "")

# Read the barrnap files and count the lines
df = read.table( params$path_barrnap_sum, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# keep only ASV_ID & eval columns & sort
df <- subset(df, select = c(ASV_ID,mito_eval,euk_eval,arc_eval,bac_eval))
# choose kingdom (column) with lowest evalue
df[is.na(df)] <- 1
df$result = colnames(df[,2:5])[apply(df[,2:5],1,which.min)]
df$result = gsub("_eval", "", df$result)

#import asv table
asv_table <- read.table(file = params$asv_table_path, header = TRUE, sep = "\t")
n_asv <- length(asv_table$ASV_ID)

# calculate numbers
n_classified <- length(df$result)
n_bac <- sum(grepl("bac", df$result))
n_arc <- sum(grepl("arc", df$result))
n_mito <- sum(grepl("mito", df$result))
n_euk <- sum(grepl("euk", df$result))

df_sum <- data.frame(label=c('Bacteria','Archea','Mitochondria','Eukaryotes','Unclassified'),
                 count=c(n_bac,n_arc,n_mito,n_euk,n_asv - n_classified),
                 percent=c(round( (n_bac/n_asv)*100, 2), round( (n_arc/n_asv)*100, 2), round( (n_mito/n_asv)*100, 2), round( (n_euk/n_asv)*100, 2), round( ( (n_asv - n_classified) /n_asv)*100, 2) ) )

# Build outputtext
cat( "Barrnap classified ")
cat( df_sum$count[1], "(", df_sum$percent[1],"%) ASVs as most similar to Bacteria, " )
cat( df_sum$count[2], "(", df_sum$percent[2],"%) ASVs to Archea, " )
cat( df_sum$count[3], "(", df_sum$percent[3],"%) ASVs to Mitochondria, " )
cat( df_sum$count[4], "(", df_sum$percent[4],"%) ASVs to Eukaryotes, and" )
cat( df_sum$count[5], "(", df_sum$percent[5],"%) were below similarity threshold to any kingdom." )

# Barplot
ggplot(df_sum,
        aes(x = reorder(label, desc(label)), y = percent)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("rRNA origins") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_dada2_taxonomy, results='asis'}
# Header
cat("# Taxonomic Classification\n")
cat("## Taxonomic Classification using DADA2\n")

if (!params$flag_ref_tax_user) {
    ref_tax <- readLines(params$ref_tax_path)

    db <- "Unknown DB"
    for (line in ref_tax){
            if (grepl("Title:", line)) {
                    db <- sub(".*Title: ", "", line)
            }
    }

    # Output text db
    cat("The taxonomic classification was performed by DADA2 using the database: ",
            "\"", db, "\".\n\n", sep = "")
} else {
    # Output text db
    cat("The taxonomic classification was performed by DADA2 using a custom database ",
            "provided by the user.\n\n", sep = "")
}

asv_tax <- read.table(params$dada2_taxonomy, header = TRUE, sep = "\t")

# Calculate the classified numbers/percent of asv
level <- subset(asv_tax, select = -c(ASV_ID,confidence,sequence))
level <- colnames(level)

# Catch 100% highest taxa (e.g. Kingdom) assignment
if (count(asv_tax, level[1])$n[1] == nrow(asv_tax)){
    n_1 = 0
} else {
    n_1 = count(asv_tax, level[1])$n[1]
}
n_asv_tax = nrow(asv_tax)
n_asv_unclassified <- c(n_1)
for (x in level[2:length(level)]) {
    asv_tax_subset <- subset(asv_tax, select = x)
    colnames(asv_tax_subset)[1] <- "count_this"
    n_asv_unclassified <- c(n_asv_unclassified, count(asv_tax_subset, count_this)$n[1])
}

n_asv_classified <- n_asv_tax - n_asv_unclassified
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "DADA2 classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_qiime2_taxonomy, results='asis'}
# Header #TODO: that header line needs to be also displayed with other taxonomic assignments!
cat("## Taxonomic Classification using QIIME2\n")

#TODO: add database information
#TODO: only tested for greengenes85, need to test also UNITE and SILVA!

# Read file and prepare table
asv_tax <- read.table(params$qiime2_taxonomy, header = TRUE, sep = "\t")
#asv_tax <- data.frame(do.call('rbind', strsplit(as.character(asv_tax$Taxon),'; ',fixed=TRUE)))
asv_tax <- subset(asv_tax, select = Taxon)

# Remove greengenes85 ".__" placeholders
df = as.data.frame(lapply(asv_tax, function(x) gsub(".__", "", x)))
# remove all last, empty ;
df = as.data.frame(lapply(df, function(x) gsub(" ;","",x)))
# remove last remaining, empty ;
df = as.data.frame(lapply(df, function(x) gsub("; $","",x)))

# get maximum amount of taxa levels per ASV
max_taxa <- lengths(regmatches(df$Taxon, gregexpr("; ", df$Taxon)))+1

# Currently, all QIIME2 databases seem to have the same levels!
level <- c("Kingdom","Phylum","Class","Order","Family","Genus","Species")

# Calculate the classified numbers/percent of asv
n_asv_tax = nrow(asv_tax)

n_asv_classified <- length(which(max_taxa>=1))
for (x in 2:length(level)) {
    n_asv_classified <- c(n_asv_classified, length(which(max_taxa>=x)) )
}
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "QIIME2 classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_sintax_taxonomy, results='asis'}
# Header
cat("## Taxonomic Classification using SINTAX\n")

#TODO: add database information

asv_tax <- read.table(params$sintax_taxonomy, header = TRUE, sep = "\t")

# Calculate the classified numbers/percent of asv
level <- subset(asv_tax, select = -c(ASV_ID,confidence,sequence))
level <- colnames(level)

# Catch 100% highest taxa (e.g. Kingdom) assignment
if (count(asv_tax, level[1])$n[1] == nrow(asv_tax)){
    n_1 = nrow(asv_tax)
} else {
    n_1 = count(asv_tax, level[1])$n[1]
}
n_asv_tax = nrow(asv_tax)
n_asv_unclassified <- c(n_1)
for (x in level[2:length(level)]) {
    asv_tax_subset <- subset(asv_tax, select = x)
    colnames(asv_tax_subset)[1] <- "count_this"
    n_asv_unclassified <- c(n_asv_unclassified, count(asv_tax_subset, count_this)$n[1])
}

n_asv_classified <- n_asv_tax - n_asv_unclassified
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "SINTAX classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_pplace_taxonomy, results='asis'}
# Header
cat("## Taxonomic Classification using Phylogenetic Placement\n")

#TODO: add database information
#TODO: only tested for greengenes85, need to test also UNITE and SILVA!

# Read file and prepare table
asv_tax <- read.table(params$pplace_taxonomy, header = TRUE, sep = "\t")

# get maximum amount of taxa levels per ASV
max_taxa <- lengths(regmatches(asv_tax$taxonomy, gregexpr(";", asv_tax$taxonomy)))+1

# labels for levels
level <- rep(1:max(max_taxa))

# Calculate the classified numbers/percent of asv
n_asv_tax = nrow(asv_tax)

n_asv_classified <- length(which(max_taxa>=1))
for (x in 2:length(level)) {
    n_asv_classified <- c(n_asv_classified, length(which(max_taxa>=x)) )
}
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "Phylogenetic Placement classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```
