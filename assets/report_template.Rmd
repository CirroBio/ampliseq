---
title: Summary of nf-core/ampliseq results
output:
    html_document:
        toc: true # table of contents
        toc_float: true # float the table of contents to the left of the main document content
        toc_depth: 3 # header levels 1,2,3
        theme: default
        number_sections: true # add section numbering to headers
        df_print: paged # tables are printed as an html table with support for pagination over rows and columns
        css: ./report_styles.css
        highlight: pygments
    pdf_document: true
#bibliography: ./references.bibtex
params:
    # flags and arguments
    flag_skip_fastqc: FALSE
    flag_skip_cutadapt: FALSE
    flag_skip_dada_quality: FALSE
    flag_skip_barrnap: FALSE
    #flag_skip_taxonomy: FALSE
    flag_retain_untrimmed: TRUE
    flag_ref_tax_user: FALSE
    flag_single_end: FALSE
    flag_dada2_taxonomy: FALSE
    flag_qiime2_taxonomy: FALSE
    flag_sintax_taxonomy: FALSE
    flag_pplace_taxonomy: FALSE
    trunclenf: ""
    trunclenr: ""
    max_ee: ""
    trunc_qmin: ""
    trunc_rmin: ""
    min_len_asv: ""
    max_len_asv: ""

    # file paths
    mqc_plot: ""
    ca_sum_path: ""
    dada_filtntrim_args: ""
    dada_qc_f_path: ""
    dada_qc_r_path: ""
    dada_pp_qc_f_path: ""
    dada_pp_qc_r_path: ""
    dada_1_err_path: ""
    dada_2_err_path: ""
    asv_table_path: ""
    path_asv_fa: ""
    path_dada2_tab: ""
    dada_stats_path: ""
    path_rrna_arc: ""
    path_rrna_bac: ""
    path_rrna_euk: ""
    path_rrna_mito: ""
    path_barrnap_sum: ""
    filter_len_asv: ""
    filter_len_asv_len_orig: ""
    filter_codons: ""
    stop_codons: ""
    ref_tax_path: ""
    dada2_taxonomy: ""
    sintax_taxonomy: ""
    pplace_taxonomy: ""
    qiime2_taxonomy: ""

---

```{r setup, include=FALSE}
library("dplyr")
library("ggplot2")
library("knitr")
library("DT")
library("formattable")
library("purrr")
knitr::opts_chunk$set(echo = FALSE)
```

# Preprocessing

```{r, eval = !params$flag_skip_fastqc, results='asis'}
mqc_rep_path <- paste0("../multiqc/multiqc_report.html")

cat("## FastQC\n")
cat("FastQC gives general quality metrics about your sequenced reads. ",
    "It provides information about the quality score distribution across your reads, ",
    "per base sequence content (%A/T/G/C), adapter contamination and overrepresented sequences.\n")
cat("The sequence quality was checked using FastQC and resulting data was ",
    "aggregated using the FastQC module of MultiQC. For more quality ",
    "controls and per sample quality checks you can check the full ",
    "MultiQC report, which is found [here](", mqc_rep_path, ").", sep = "")
```

```{r, eval = !params$flag_skip_fastqc, out.width='100%', dpi=1200, fig.align='center'}
knitr::include_graphics(params$mqc_plot)
```

```{r, eval = !params$flag_skip_cutadapt, results='asis'}
cat("## Primer removal with Cutadapt\n")
cat("Cutadapt is trimming primer sequences from sequencing reads. ",
    "Primer sequences are non-biological sequences that often introduce ",
    "point mutations that do not reflect sample sequences. This is especially ",
    "true for degenerated PCR primer. If primer trimming would be omitted, artifactual ",
    "amplicon sequence variants might be computed by the denoising tool or ",
    "sequences might be lost due to become labelled as PCR chimera.\n\n")

# import tsv
cutadapt_summary <- read.table(file = params$ca_sum_path, header = TRUE, sep = "\t")

passed_col <- as.numeric(substr(
        cutadapt_summary$cutadapt_passing_filters_percent, 1, 4))

max_disc <- 100 - min(passed_col)
avg_passed <- round(mean(passed_col),1)

cutadapt_text_unch <- "Primers were trimmed using cutadapt"
cutadapt_text_ch <- paste0(" and all untrimmed sequences were discarded. ",
    "Sequences that did not contain primer sequences were considered artifacts. Less than ",
    max_disc, "% of the sequences were discarded per sample and a mean of ",
    avg_passed, "% of the sequences per sample passed the filtering.")

if (!params$flag_retain_untrimmed) cutadapt_text <- paste0(
    cutadapt_text_unch, cutadapt_text_ch
    ) else cutadapt_text <- paste0(cutadapt_text_unch, ".")

cat(cutadapt_text)

# shorten header by "cutadapt_" to optimize visualisation
colnames(cutadapt_summary) <- gsub("cutadapt_","",colnames(cutadapt_summary))

datatable(cutadapt_summary, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))

# Barplot TODO: currently skipper, because this is already in the table
#cutadapt_summary$passed_num <- passed_col
#ggplot(cutadapt_summary,
#        aes(x = sample, y = passed_col)) +
#        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
#        ylab("% sequencing reads passing filters of cutadapt") +
#        xlab("Samples") +
#        coord_flip() +
#        theme_bw()
```

## Quality filtering using DADA2

```{r, results='asis'}
cat("Additional quality filtering can improve sequence recovery. ",
    "Often it is advised trimming the last few nucleotides to avoid less well-controlled errors that can arise there. ")

if (params$trunc_qmin != -1) {
    f_and_tr_args <- readLines(params$dada_filtntrim_args)
    trunc_len <- strsplit(gsub(".*truncLen = c\\((.+)\\),maxN.*", "\\1",
                            f_and_tr_args), ", ")
    tr_len_f <- trunc_len[[1]][1]
    tr_len_r <- trunc_len[[1]][2]
    cat("Reads were trimmed before median quality drops ",
        "below ", params$trunc_qmin, " and at least ",params$trunc_rmin*100,
        "% of reads are retained, resulting in a trim of ",
        "forward reads at ", tr_len_f, " bp and reverse ",
        "reads at ", tr_len_r, " bp, reads shorter than this are discarded. ", sep = "")
} else if (params$trunclenf == "null" && params$trunclenr == "null") {
    cat("Reads were not trimmed.")
} else if (params$trunclenf != 0 && params$trunclenr != 0) {
    cat("Forward reads were trimmed at ", params$trunclenf,
            " bp and reverse reads were trimmed at ", params$trunclenr,
            " bp, reads shorter than this are discarded. ", sep = "")
} else if (params$trunclenf != 0) {
    cat("Forward reads were trimmed at ", params$trunclenf," bp, reads shorter than this are discarded. ", sep = "")
} else if (params$trunclenr != 0) {
    cat("Reverse reads were trimmed at ", params$trunclenr," bp, reads shorter than this are discarded. ", sep = "")
}
cat("Reads with more than", params$max_ee,"expected errors were discarded.", sep = " ")
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
cat ("**Quality profiles:**\n\n")

if (params$flag_single_end) {
    cat("Read quality stats for incoming data:")
} else {
    cat("Forward (left) and reverse (right) read quality stats for incoming data:")
}
```

```{r, eval = !params$flag_skip_dada_quality, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_qc_f_path, params$dada_qc_r_path))
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
if (params$flag_single_end) {
    cat("Read quality stats for preprocessed data:")
} else {
    cat("Forward (left) and reverse (right) read quality stats for preprocessed data:")
}
```

```{r, eval = !params$flag_skip_dada_quality, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_pp_qc_f_path, params$dada_pp_qc_r_path))
```

```{r, eval = !params$flag_skip_dada_quality, results='asis'}
cat("Overall read quality profiles as heat map of the frequency of each quality score at each base position. ",
    "The mean quality score at each position is shown by the green line, and the quartiles of the quality score ",
    "distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least ",
    "that position. Original plots can be found [folder dada2/QC/](../dada2/QC/) with names that end in '_qual_stats.pdf'.")
```

# ASV inference using DADA2

DADA2 performs fast and accurate sample inference from amplicon data with single-nucleotide resolution.
It infers exact amplicon sequence variants (ASVs) from amplicon data with fewer false positives than
many other methods while maintaining high sensitivity.

DADA2 reduces sequence errors and dereplicates sequences by quality filtering, denoising,
read pair merging (for paired end Illumina reads only) and PCR chimera removal.

## Error correction

Read error correction was performed using estimated error rates, visualized below.

```{r, out.width="49%", fig.show='hold', fig.align='default'}
knitr::include_graphics(c(params$dada_1_err_path, params$dada_2_err_path))
```

Estimated error rates for each possible transition. The black line shows the estimated error rates after
convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal
definition of the Q-score. The estimated error rates (black line) should be a good fit to the observed rates
(points), and the error rates should drop with increased quality. Original plots can be found in
[folder dada2/QC/](../dada2/QC/) with names that end in '.err.pdf'.

## Read counts per sample

Tracking read numbers through DADA2 processing steps, for each sample. In the following table are read numbers after each processing stage.

```{r, results='asis'}
if ( params$flag_single_end ) {
    cat("Processing stages are: input - reads into DADA2, filtered - reads passed quality filtering, ",
        "denoised - reads after denoising, nonchim - reads in non-chimeric sequences (final ASVs)")
} else {
    cat("Processing stages are: input - read pairs into DADA2, filtered - read pairs passed quality filtering, ",
        "denoisedF - forward reads after denoising, denoisedR - reverse reads after denoising, ",
        "merged - successfully merged read pairs, nonchim - read pairs in non-chimeric sequences (final ASVs)")
}

# import stats tsv
dada_stats <- read.table(file = params$dada_stats_path, header = TRUE, sep = "\t")

# Display table
datatable(dada_stats, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))
```
Samples with unusual low reads numbers relative to the number of expected ASVs (e.g. 500 reads with 100 ASVs)
should be treated cautiously, because the abundance estimate will be very granular and might vary strongly between (theoretical)
replicates due to high impact of stochasticity.

Stacked barcharts of read numbers per sample and processing stage (see above):

```{r, results='asis'}
# Stacked barchart to num of reads

# Calc exluded asvs and transform all cols to percent

if ( params$flag_single_end ) {
    # single end
    dada_stats_ex <- data.frame(sample = dada_stats$sample,
                            input = dada_stats$DADA2_input,
                            filtered = dada_stats$DADA2_input-dada_stats$filtered,
                            denoised = dada_stats$filtered-dada_stats$denoised,
                            nonchim = dada_stats$denoised-dada_stats$nonchim,
                            analysis = dada_stats$nonchim)
    dada_stats_p <- data.frame(sample = dada_stats_ex$sample, round(dada_stats_ex[2:6]/dada_stats_ex$input, 2))
    n_samples <- length(dada_stats_p$sample)
    # Stack columns for both stacked barcharts
    samples_t <- c(rep(dada_stats_p$sample, 4))
    steps_t <- c(rep("excluded by filtering", n_samples), rep("excluded by denoised", n_samples),
                rep("excluded by nonchim", n_samples), rep("reads in final ASVs", n_samples))
    # stack the column for absolute number of asvs
    asvs_abs_t <- as.array(flatten_dbl(dada_stats_ex[3:6]))
    dada_stats_ex_t <- data.frame(samples_t, steps_t, asvs_abs_t)
    # stack the column for percentage of asvs
    asvs_p_t <- as.array(flatten_dbl(dada_stats_p[3:6]))
    dada_stats_p_t <- data.frame(samples_t, steps_t, asvs_p_t)
} else {
    # paired end
    dada_stats_ex <- data.frame(sample = dada_stats$sample,
                            DADA2_input = dada_stats$DADA2_input,
                            filtered = dada_stats$DADA2_input-dada_stats$filtered,
                            denoisedF = dada_stats$filtered-dada_stats$denoisedF,
                            denoisedR = dada_stats$denoisedF-dada_stats$denoisedR,
                            merged = dada_stats$denoisedR-dada_stats$merged,
                            nonchim = dada_stats$merged-dada_stats$nonchim,
                            analysis = dada_stats$nonchim)
    dada_stats_p <- data.frame(sample = dada_stats_ex$sample, round(dada_stats_ex[2:8]/dada_stats_ex$DADA2_input, 2))
    # Stack columns for both stacked barcharts
    n_samples <- length(dada_stats_p$sample)
    samples_t <- c(rep(dada_stats_p$sample, 6))
    steps_t <- c(rep("excluded by filtering", n_samples), rep("excluded by denoisedF", n_samples),
                rep("excluded by denoisedR", n_samples), rep("excluded by merged", n_samples),
                rep("excluded by nonchim", n_samples), rep("reads in final ASVs", n_samples))
    # stack the column for absolute reads
    asvs_abs_t <- as.array(flatten_dbl(dada_stats_ex[3:8]))
    dada_stats_ex_t <- data.frame(samples_t, steps_t, asvs_abs_t)
    # stack the column for percentage of asvs
    asvs_p_t <- as.array(flatten_dbl(dada_stats_p[3:8]))
    dada_stats_p_t <- data.frame(samples_t, steps_t, asvs_p_t)
}

# Plot
#dada_stats_ex_t$steps_t <- factor(dada_stats_ex_t$steps_t, levels=unique(dada_stats_ex_t$steps_t))
#ggplot(dada_stats_ex_t, aes(fill = steps_t, y = asvs_abs_t, x = samples_t)) +
#        geom_bar(position = "stack", stat = "identity") +
#        xlab("Samples") +
#        ylab("Absolute reads") +
#        coord_flip() +
#        scale_fill_brewer("Filtering Steps", palette = "Spectral")

# Plot
dada_stats_p_t$steps_t <- factor(dada_stats_p_t$steps_t, levels=unique(dada_stats_p_t$steps_t))
ggplot(dada_stats_p_t, aes(fill = steps_t, y = asvs_p_t, x = samples_t)) +
        geom_bar(position = "fill", stat = "identity") +
        xlab("Samples") +
        ylab("% of total reads") +
        coord_flip() +
        scale_fill_brewer("Filtering Steps", palette = "Spectral")
```

The proportion of lost reads per processing stage and sample should not be too high, totalling typically <50%.
Samples that are very different in lost reads (per stage) to the majority of samples must be compared with caution, because an unusual problem
(e.g. during nucleotide extraction, library preparation, or sequencing) could have occurred that might add bias to the analysis.

## Inferred ASVs

```{r, results='asis'}
#import asv table
asv_table <- read.table(file = params$asv_table_path, header = TRUE, sep = "\t")
n_asv <- length(asv_table$ASV_ID)

# Output text
cat("Finally,", n_asv,
        "amplicon sequence variants (ASVs) were obtained across all samples. ")
cat("The ASVs can be found in ['dada2/ASV_seqs.fasta'](../dada2/). And the corresponding",
        " quantification of the ASVs across samples is in",
        "['dada2/ASV_table.tsv'](../dada2/). An extensive table containing both was ",
        "saved as ['dada2/DADA2_table.tsv'](../dada2/)")
```

```{r, results='asis'}
flag_any_filtering <- !params$flag_skip_barrnap || isTRUE(params$filter_len_asv != "") || isTRUE(params$filter_codons != "")
```

```{r, eval = flag_any_filtering, results='asis'}
cat("# Filtering of ASVs\n")
```

```{r, eval = !params$flag_skip_barrnap, results='asis'}
cat("## rRNA detection\n")
cat("Barrnap classifies the ASVs into the origin domain (including ",
        "mitochondiral origin). Using this classification the ASVs can ",
        "be filtered by sample origin.\n\n", sep = "")

# Read the barrnap files and count the lines
df = read.table( params$path_barrnap_sum, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# keep only ASV_ID & eval columns & sort
df <- subset(df, select = c(ASV_ID,mito_eval,euk_eval,arc_eval,bac_eval))
# choose kingdom (column) with lowest evalue
df[is.na(df)] <- 1
df$result = colnames(df[,2:5])[apply(df[,2:5],1,which.min)]
df$result = gsub("_eval", "", df$result)

#import asv table
asv_table <- read.table(file = params$asv_table_path, header = TRUE, sep = "\t")
n_asv <- length(asv_table$ASV_ID)

# calculate numbers
n_classified <- length(df$result)
n_bac <- sum(grepl("bac", df$result))
n_arc <- sum(grepl("arc", df$result))
n_mito <- sum(grepl("mito", df$result))
n_euk <- sum(grepl("euk", df$result))

df_sum <- data.frame(label=c('Bacteria','Archea','Mitochondria','Eukaryotes','Unclassified'),
                 count=c(n_bac,n_arc,n_mito,n_euk,n_asv - n_classified),
                 percent=c(round( (n_bac/n_asv)*100, 2), round( (n_arc/n_asv)*100, 2), round( (n_mito/n_asv)*100, 2), round( (n_euk/n_asv)*100, 2), round( ( (n_asv - n_classified) /n_asv)*100, 2) ) )

# Build outputtext
cat( "Barrnap classified ")
cat( df_sum$count[1], "(", df_sum$percent[1],"%) ASVs as most similar to Bacteria, " )
cat( df_sum$count[2], "(", df_sum$percent[2],"%) ASVs to Archea, " )
cat( df_sum$count[3], "(", df_sum$percent[3],"%) ASVs to Mitochondria, " )
cat( df_sum$count[4], "(", df_sum$percent[4],"%) ASVs to Eukaryotes, and " )
cat( df_sum$count[5], "(", df_sum$percent[5],"%) were below similarity threshold to any kingdom." )

# Barplot
ggplot(df_sum,
        aes(x = reorder(label, desc(label)), y = percent)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("rRNA origins") +
        coord_flip() +
        theme_bw()

#TODO: add reads/ASVs removed as below?!
```

```{r, results='asis'}
flag_filter_len_asv <- isTRUE(params$filter_len_asv != "")
```

```{r, eval = flag_filter_len_asv, results='asis'}
cat("## Sequence length\n")

cat("A length filter was used to reduce potential contamination after ASV computation.",
    "Before filtering, ASVs had the following length profile:\n\n")

# ASV length profile

# import length profile tsv
filter_len_profile <- read.table(file = params$filter_len_asv_len_orig, header = TRUE, sep = "\t")

# find number of ASVs filtered
filter_len_asv_filtered <- filter_len_profile
if ( params$min_len_asv != 0 ) {
    filter_len_asv_filtered <- subset(filter_len_asv_filtered, Length >= params$min_len_asv)
}
if ( params$max_len_asv != 0 ) {
    filter_len_asv_filtered <- subset(filter_len_asv_filtered, Length <= params$max_len_asv)
}

ggplot(filter_len_profile,
        aes(x = Length, y = Counts)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("Number of ASVs") +
        xlab("Length") +
        coord_flip() +
        theme_bw()

# Reads removed

# import stats tsv
filter_len_stats <- read.table(file = params$filter_len_asv, header = TRUE, sep = "\t")
# re-name & re-order columns
colnames(filter_len_stats) <- gsub("lenfilter_","",colnames(filter_len_stats))
filter_len_stats <- filter_len_stats[, c("sample", "input", "output")]
filter_len_stats$'retained%' <- round( filter_len_stats$output / filter_len_stats$input * 100 , 2)
filter_len_stats_avg_removed <- 100-sum(filter_len_stats$'retained%')/length(filter_len_stats$'retained%')
filter_len_stats_max_removed <- 100-min(filter_len_stats$'retained%')

cat("\n\n")
if ( params$min_len_asv != 0 && params$max_len_asv != 0 ) {
    cat("Filtering omitted all ASVs with length lower than",params$min_len_asv,"or above",params$max_len_asv,"bp. ")
} else if ( params$min_len_asv != 0 ) {
    cat("Filtering omitted all ASVs with length lower than",params$min_len_asv,"bp. ")
} else if ( params$max_len_asv != 0 ) {
    cat("Filtering omitted all ASVs with length above",params$max_len_asv,"bp. ")
}
cat("The following table shows (read) counts for each sample before and after filtering:")

# Display table
datatable(filter_len_stats, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))

cat("In average", filter_len_stats_avg_removed, "% reads were removed, but at most",filter_len_stats_max_removed,"% reads per sample. ")
cat("The number of ASVs was reduced by",sum(filter_len_profile$Counts)-sum(filter_len_asv_filtered$Counts),"(",100-round( sum(filter_len_asv_filtered$Counts)/sum(filter_len_profile$Counts)*100 ,2),"%), from",sum(filter_len_profile$Counts),"to",sum(filter_len_asv_filtered$Counts)," ASVs.")
```

```{r, results='asis'}
flag_filter_codons <- isTRUE(params$filter_codons != "")
```

```{r, eval = flag_filter_codons, results='asis'}
cat("## Codon usage\n")

cat("Amplicons of coding regions are expected to be free of stop codons and consist of condon tripletts.",
    "ASVs were filtered against the presence of stop codons (",params$stop_codons,") in the specified open reading frame of the ASV.",
    "Additionally, ASVs that are not multiple of 3 in length were omitted.\n\n")

# import stats tsv
filter_codons_stats <- read.table(file = params$filter_codons, header = TRUE, sep = "\t")

cat("The following table shows read counts for each sample after filtering:")

# Display table
datatable(filter_codons_stats, options = list(
        scrollX = TRUE,
        scrollY = "300px",
        paging = FALSE))

#TODO: add ASV count after filtering
```

```{r, results='asis'}
# Check if any taxonomic classification is available
any_taxonomy <- params$flag_dada2_taxonomy || params$flag_qiime2_taxonomy || params$flag_sintax_taxonomy || params$flag_pplace_taxonomy
```

```{r, eval = any_taxonomy, results='asis'}
# Header if any taxonomic classification is available
cat("# Taxonomic Classification\n")
```

```{r, eval = params$flag_dada2_taxonomy, results='asis'}
cat("## Taxonomic Classification using DADA2\n")

if (!params$flag_ref_tax_user) {
    ref_tax <- readLines(params$ref_tax_path)

    db <- "Unknown DB"
    for (line in ref_tax){
            if (grepl("Title:", line)) {
                    db <- sub(".*Title: ", "", line)
            }
    }

    # Output text db
    cat("The taxonomic classification was performed by DADA2 using the database: ",
            "\"", db, "\".\n\n", sep = "")
} else {
    # Output text db
    cat("The taxonomic classification was performed by DADA2 using a custom database ",
            "provided by the user.\n\n", sep = "")
}

asv_tax <- read.table(params$dada2_taxonomy, header = TRUE, sep = "\t")

# Calculate the classified numbers/percent of asv
level <- subset(asv_tax, select = -c(ASV_ID,confidence,sequence))
level <- colnames(level)

# Catch 100% highest taxa (e.g. Kingdom) assignment
if (count(asv_tax, level[1])$n[1] == nrow(asv_tax)){
    n_1 = 0
} else {
    n_1 = count(asv_tax, level[1])$n[1]
}
n_asv_tax = nrow(asv_tax)
n_asv_unclassified <- c(n_1)
for (x in level[2:length(level)]) {
    asv_tax_subset <- subset(asv_tax, select = x)
    colnames(asv_tax_subset)[1] <- "count_this"
    n_asv_unclassified <- c(n_asv_unclassified, count(asv_tax_subset, count_this)$n[1])
}

n_asv_classified <- n_asv_tax - n_asv_unclassified
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "DADA2 classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_qiime2_taxonomy, results='asis'}
# Header
cat("## Taxonomic Classification using QIIME2\n")

#TODO: add database information
#TODO: only tested for greengenes85, need to test also UNITE and SILVA!

# Read file and prepare table
asv_tax <- read.table(params$qiime2_taxonomy, header = TRUE, sep = "\t")
#asv_tax <- data.frame(do.call('rbind', strsplit(as.character(asv_tax$Taxon),'; ',fixed=TRUE)))
asv_tax <- subset(asv_tax, select = Taxon)

# Remove greengenes85 ".__" placeholders
df = as.data.frame(lapply(asv_tax, function(x) gsub(".__", "", x)))
# remove all last, empty ;
df = as.data.frame(lapply(df, function(x) gsub(" ;","",x)))
# remove last remaining, empty ;
df = as.data.frame(lapply(df, function(x) gsub("; $","",x)))

# get maximum amount of taxa levels per ASV
max_taxa <- lengths(regmatches(df$Taxon, gregexpr("; ", df$Taxon)))+1

# Currently, all QIIME2 databases seem to have the same levels!
level <- c("Kingdom","Phylum","Class","Order","Family","Genus","Species")

# Calculate the classified numbers/percent of asv
n_asv_tax = nrow(asv_tax)

n_asv_classified <- length(which(max_taxa>=1))
for (x in 2:length(level)) {
    n_asv_classified <- c(n_asv_classified, length(which(max_taxa>=x)) )
}
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "QIIME2 classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_sintax_taxonomy, results='asis'}
# Header
cat("## Taxonomic Classification using SINTAX\n")

#TODO: add database information

asv_tax <- read.table(params$sintax_taxonomy, header = TRUE, sep = "\t")

# Calculate the classified numbers/percent of asv
level <- subset(asv_tax, select = -c(ASV_ID,confidence,sequence))
level <- colnames(level)

# Catch 100% highest taxa (e.g. Kingdom) assignment
if (count(asv_tax, level[1])$n[1] == nrow(asv_tax)){
    n_1 = nrow(asv_tax)
} else {
    n_1 = count(asv_tax, level[1])$n[1]
}
n_asv_tax = nrow(asv_tax)
n_asv_unclassified <- c(n_1)
for (x in level[2:length(level)]) {
    asv_tax_subset <- subset(asv_tax, select = x)
    colnames(asv_tax_subset)[1] <- "count_this"
    n_asv_unclassified <- c(n_asv_unclassified, count(asv_tax_subset, count_this)$n[1])
}

n_asv_classified <- n_asv_tax - n_asv_unclassified
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "SINTAX classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at ", asv_classi_df[row, ]$level, " level, ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Levels") +
        coord_flip() +
        theme_bw()
```

```{r, eval = params$flag_pplace_taxonomy, results='asis'}
# Header
cat("## Taxonomic Classification using Phylogenetic Placement\n")

#TODO: add database information

# Read file and prepare table
asv_tax <- read.table(params$pplace_taxonomy, header = TRUE, sep = "\t")

# get maximum amount of taxa levels per ASV
max_taxa <- lengths(regmatches(asv_tax$taxonomy, gregexpr(";", asv_tax$taxonomy)))+1

# labels for levels
level <- rep(1:max(max_taxa))

# Calculate the classified numbers/percent of asv
n_asv_tax = nrow(asv_tax)

n_asv_classified <- length(which(max_taxa>=1))
for (x in 2:length(level)) {
    n_asv_classified <- c(n_asv_classified, length(which(max_taxa>=x)) )
}
p_asv_classified <- round(n_asv_classified / n_asv_tax * 100, 2)

asv_classi_df <- data.frame(level, n_asv_classified, p_asv_classified)

# Build output string
outputstr <- "Phylogenetic Placement classified "
for (row in seq_len(nrow(asv_classi_df))) {
        outputstr <- paste0(outputstr, asv_classi_df[row, ]$p_asv_classified,
                         " % ASVs at taxonomic level ", asv_classi_df[row, ]$level, ", ")
}
outputstr <- substr(outputstr, 1, nchar(outputstr)-2)
outputstr <- paste0(outputstr, ".\n\n")

# Output Text Classifications
cat(outputstr)

# Barplot
# Plot
asv_classi_df$level <- factor(asv_classi_df$level, levels = asv_classi_df$level)
ggplot(asv_classi_df,
        aes(x = reorder(level, desc(level)), y = p_asv_classified)) +
        geom_bar(stat = "identity", fill = rgb(0.1, 0.4, 0.75), width = 0.5) +
        ylab("% Classification") +
        xlab("Taxonomic levels") +
        coord_flip() +
        theme_bw()
```
